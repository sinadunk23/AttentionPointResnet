{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'data_utils')\n",
    "sys.path.insert(1, 'models/')\n",
    "\n",
    "from transform_functions import PCRNetTransform as transform\n",
    "import transform_functions\n",
    "from modelnet_reg_utils import ModelNet40Data, RegistrationData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import time\n",
    "from models.feature_models import PointResNet, PointNet, AttentionPointResNet\n",
    "from models.attention_pooling import AttentionPooling\n",
    "from utils.load_model import load_model\n",
    "from args import Args\n",
    "import transforms3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "arger = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_model(nn.Module):\n",
    "    def __init__(self,args,  feature_extractor = PointResNet):\n",
    "        super(get_model, self).__init__()\n",
    "        self.args = args\n",
    "        self.feature_extractor = feature_extractor(self.args)\n",
    "        if self.args.attention_pooling:\n",
    "            self.attentional_pooling = AttentionPooling()\n",
    "        else:\n",
    "            self.attentional_pooling = None\n",
    "    def forward(self, x):\n",
    "        x_ap, x_mp = self.feature_extractor(x)\n",
    "        if self.args.attention_pooling:\n",
    "            x_feat= self.attentional_pooling(x_ap, x_mp)\n",
    "        else:\n",
    "            x_feat = x_mp\n",
    "        return x_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iPCRNet(nn.Module):\n",
    "\tdef __init__(self, feature_model, droput=0.0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.feature_model = feature_model\n",
    "\n",
    "\t\tself.linear = [nn.Linear(1024 * 2, 1024), nn.ReLU(),\n",
    "\t\t\t\t   \t   nn.Linear(1024, 1024), nn.ReLU(),\n",
    "\t\t\t\t   \t   nn.Linear(1024, 512), nn.ReLU(),\n",
    "\t\t\t\t   \t   nn.Linear(512, 512), nn.ReLU(),\n",
    "\t\t\t\t   \t   nn.Linear(512, 256), nn.ReLU()]\n",
    "\n",
    "\t\tif droput>0.0:\n",
    "\t\t\tself.linear.append(nn.Dropout(droput))\n",
    "\t\tself.linear.append(nn.Linear(256,7))\n",
    "\n",
    "\t\tself.linear = nn.Sequential(*self.linear)\n",
    "\n",
    "\t# Single Pass Alignment Module (SPAM)\n",
    "\tdef spam(self, template_features, source, est_R, est_t):\n",
    "\t\tbatch_size = source.size(0)\n",
    "\t\tself.source_features = self.feature_model(source.permute(0,2,1))\n",
    "\t\ty = torch.cat([template_features, self.source_features], dim=1)\n",
    "\t\tpose_7d = self.linear(y)\n",
    "\t\tpose_7d = transform.create_pose_7d(pose_7d)\n",
    "\n",
    "\t\t# Find current rotation and translation.\n",
    "\t\tidentity = torch.eye(3).to(source).view(1,3,3).expand(batch_size, 3, 3).contiguous()\n",
    "\t\test_R_temp = transform.quaternion_rotate(identity, pose_7d).permute(0, 2, 1)\n",
    "\t\test_t_temp = transform.get_translation(pose_7d).view(-1, 1, 3)\n",
    "\n",
    "\t\t# update translation matrix.\n",
    "\t\test_t = torch.bmm(est_R_temp, est_t.permute(0, 2, 1)).permute(0, 2, 1) + est_t_temp\n",
    "\t\t# update rotation matrix.\n",
    "\t\test_R = torch.bmm(est_R_temp, est_R)\n",
    "\t\t\n",
    "\t\tsource = transform.quaternion_transform(source, pose_7d)      # Ps' = est_R*Ps + est_t\n",
    "\t\treturn est_R, est_t, source\n",
    "\n",
    "\tdef forward(self, template, source, max_iteration=3):\n",
    "\t\test_R = torch.eye(3).to(template).view(1, 3, 3).expand(template.size(0), 3, 3).contiguous()         # (Bx3x3)\n",
    "\t\test_t = torch.zeros(1,3).to(template).view(1, 1, 3).expand(template.size(0), 1, 3).contiguous()     # (Bx1x3)\n",
    "\t\ttemplate_features = self.feature_model(template.permute(0,2,1))\n",
    "\t\tif max_iteration == 1:\n",
    "\t\t\test_R, est_t, source = self.spam(template_features, source, est_R, est_t)\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(max_iteration):\n",
    "\t\t\t\test_R, est_t, source = self.spam(template_features, source, est_R, est_t)\n",
    "\n",
    "\t\tresult = {'est_R': est_R,\t\t\t\t# source -> template\n",
    "\t\t\t\t  'est_t': est_t,\t\t\t\t# source -> template\n",
    "\t\t\t\t  'est_T': transform.convert2transformation(est_R, est_t),\t\t\t# source -> template\n",
    "\t\t\t\t  'r': template_features - self.source_features,\n",
    "\t\t\t\t  'transformed_source': source}\n",
    "\t\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "testset = RegistrationData(ModelNet40Data(train=False, download=True), is_testing=True, \n",
    "                           angle_range=90, translation_range=1, add_noise=False, shuffle_points=False)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_with_trans(source, template, trans):\n",
    "    src_b = source.detach().cpu().numpy()\n",
    "    tar_b = template.detach().cpu().numpy()\n",
    "    trans_b = trans.detach().cpu().numpy()\n",
    "\n",
    "    for i in range(src_b.shape[0]):\n",
    "        src = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(src_b[i])).paint_uniform_color([1, 0.706, 0])\n",
    "        tar = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(tar_b[i])).paint_uniform_color([0, 0.651, 0.929])\n",
    "        src.transform(trans_b[0])\n",
    "        o3d.visualization.draw_geometries([src, tar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_open3d(template, source, transformed_source):\n",
    "\ttemplate_ = o3d.geometry.PointCloud()\n",
    "\tsource_ = o3d.geometry.PointCloud()\n",
    "\ttransformed_source_ = o3d.geometry.PointCloud()\n",
    "\ttemplate_.points = o3d.utility.Vector3dVector(template)\n",
    "\tsource_.points = o3d.utility.Vector3dVector(source + np.array([0,0,0]))\n",
    "\ttransformed_source_.points = o3d.utility.Vector3dVector(transformed_source)\n",
    "\ttemplate_.paint_uniform_color([1, 0, 0])\n",
    "\tsource_.paint_uniform_color([0, 1, 0])\n",
    "\ttransformed_source_.paint_uniform_color([0, 0, 1])\n",
    "\to3d.visualization.draw_geometries([template_, source_, transformed_source_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inverse(T):\n",
    "    \"\"\"\n",
    "    Invert a batch of 4x4 transformation matrices.\n",
    "    Args:\n",
    "        T: A torch tensor of shape (B, 4, 4), where B is the batch size.\n",
    "    Returns:\n",
    "        inv_T: A torch tensor of shape (B, 4, 4), where each 4x4 matrix\n",
    "               is the inverse of the corresponding input matrix.\n",
    "    \"\"\"\n",
    "    B = T.shape[0]\n",
    "    inv_T = torch.zeros_like(T)\n",
    "    for i in range(B):\n",
    "        inv_T[i] = torch.inverse(T[i])\n",
    "    return inv_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat2mat(q):\n",
    "    quat = q[:,0:4]\n",
    "    trans = q[:,4:7]\n",
    "\n",
    "    x, y, z, w = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n",
    "\n",
    "    B = quat.size(0)\n",
    "    device = quat.device\n",
    "    \n",
    "    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n",
    "    wx, wy, wz = w*x, w*y, w*z\n",
    "    xy, xz, yz = x*y, x*z, y*z\n",
    "\n",
    "    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\n",
    "                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\n",
    "                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).reshape(B, 3, 3)\n",
    "    transMat = torch.cat([rotMat, trans.unsqueeze(-1)], dim = -1)\n",
    "    transMat = torch.cat([transMat, torch.tensor([0,0,0,1]).view(1,1,4).expand(B,1,4).to(device)], dim = 1)\n",
    "    return transMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch import to_numpy\n",
    "from common.math import se3\n",
    "from common.math_torch import se3\n",
    "from common.math.so3 import dcm2euler\n",
    "\n",
    "def compute_metrics(points_ref, points_src, gt_transforms, pred_transforms):\n",
    "    \"\"\"Compute metrics required in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def square_distance(src, dst):\n",
    "        return torch.sum((src[:, :, None, :] - dst[:, None, :, :]) ** 2, dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Euler angles, Individual translation errors (Deep Closest Point convention)\n",
    "        # TODO Change rotation to torch operations\n",
    "        r_gt_euler_deg = dcm2euler(gt_transforms[:, :3, :3].detach().cpu().numpy(), seq='xyz')\n",
    "        r_pred_euler_deg = dcm2euler(pred_transforms[:, :3, :3].detach().cpu().numpy(), seq='xyz')\n",
    "        t_gt = gt_transforms[:, :3, 3]\n",
    "        t_pred = pred_transforms[:, :3, 3]\n",
    "        r_mse = np.mean((r_gt_euler_deg - r_pred_euler_deg) ** 2, axis=1)\n",
    "        r_mae = np.mean(np.abs(r_gt_euler_deg - r_pred_euler_deg), axis=1)\n",
    "        t_mse = torch.mean((t_gt - t_pred) ** 2, dim=1)\n",
    "        t_mae = torch.mean(torch.abs(t_gt - t_pred), dim=1)\n",
    "\n",
    "        # Rotation, translation errors (isotropic, i.e. doesn't depend on error\n",
    "        # direction, which is more representative of the actual error)\n",
    "        concatenated = se3.concatenate(se3.inverse(gt_transforms), pred_transforms)\n",
    "        rot_trace = concatenated[:, 0, 0] + concatenated[:, 1, 1] + concatenated[:, 2, 2]\n",
    "        residual_rotdeg = torch.acos(torch.clamp(0.5 * (rot_trace - 1), min=-1.0, max=1.0)) * 180.0 / np.pi\n",
    "        residual_transmag = concatenated[:, :, 3].norm(dim=-1)\n",
    "\n",
    "        # Modified Chamfer distance\n",
    "        src_transformed = se3.transform(pred_transforms, points_src)\n",
    "        src_clean = se3.transform(se3.concatenate(pred_transforms, se3.inverse(gt_transforms)), points_ref)\n",
    "        dist_src = torch.min(square_distance(src_transformed, points_ref), dim=-1)[0]\n",
    "        dist_ref = torch.min(square_distance(points_ref, src_clean), dim=-1)[0]\n",
    "        chamfer_dist = torch.mean(dist_src, dim=1) + torch.mean(dist_ref, dim=1)\n",
    "\n",
    "        metrics = {\n",
    "            'l1_dist': torch.mean(torch.abs(points_ref - src_clean), dim=1),\n",
    "            'r_mse': r_mse,\n",
    "            'r_rmse': np.sqrt(r_mse),\n",
    "            'r_mae': r_mae,\n",
    "            't_mse': to_numpy(t_mse),\n",
    "            't_rmse': to_numpy(torch.sqrt(t_mse)),\n",
    "            't_mae': to_numpy(t_mae),\n",
    "            'err_r_deg': to_numpy(residual_rotdeg),\n",
    "            'err_t': to_numpy(residual_transmag),\n",
    "            'chamfer_dist': to_numpy(chamfer_dist)\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate(device, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    r_mse = []\n",
    "    t_mse = []\n",
    "    r_mae = []\n",
    "    t_mae = []\n",
    "    err_r_deg = []\n",
    "    err_t = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "            template, source, igt, igt_R, igt_t = data\n",
    "\n",
    "            template = template.to(device)\n",
    "            source = source.to(device)\n",
    "            igt = igt.to(device).squeeze(1)\n",
    "            igt_R = igt_R.to(device)\n",
    "            igt_t = igt_t.to(device)\n",
    "            source_original = source.clone()\n",
    "            template_original = template.clone()\n",
    "            igt_t = igt_t - torch.mean(source, dim=1).unsqueeze(1)\n",
    "            source = source - torch.mean(source, dim=1, keepdim=True)\n",
    "            template = template - torch.mean(template, dim=1, keepdim=True)\n",
    "\n",
    "            output = model(template, source)\n",
    "            gt_tsf = transform.convert2transformation(igt_R, igt_t).to(device)\n",
    "            pred_tsf = output['est_T']\n",
    "            est_R = output['est_R']\n",
    "            est_t = output['est_t']\n",
    "            metrics = compute_metrics(source, template, batch_inverse(gt_tsf), pred_tsf)\n",
    "            \n",
    "            r_mse.append(metrics['r_mse'])\n",
    "            t_mse.append(metrics['t_mse'])\n",
    "            r_mae.append(metrics['r_mae'])\n",
    "            t_mae.append(metrics['t_mae'])\n",
    "            err_r_deg.append(metrics['err_r_deg'])\n",
    "            err_t.append(metrics['err_t'])\n",
    "            # visualize_with_trans(source, template, pred_tsf)\n",
    "            # visualize_with_trans(source, template, batch_inverse(gt_tsf))\n",
    "            #display_open3d(template.detach().cpu().numpy()[0], source_original.detach().cpu().numpy()[0], source_tsf[0])\n",
    "        print(\"rotation mse:\", np.mean(np.array(r_mse).reshape(-1,1), axis=0))\n",
    "        print(\"translation mse:\", np.mean(np.array(t_mse).reshape(-1,1), axis=0))\n",
    "        print(\"rotation mae:\", np.mean(np.array(r_mae).reshape(-1,1), axis=0))\n",
    "        print(\"translation mae:\", np.mean(np.array(t_mse).reshape(-1,1), axis=0))\n",
    "        print(\"rotation error:\", np.mean(np.array(err_r_deg).reshape(-1,1), axis=0))\n",
    "        print(\"translation error:\", np.mean(np.array(err_t).reshape(-1,1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "else: \n",
    "    device = 'cuda:0'\n",
    "device = torch.device(device)\n",
    "\n",
    "# Create PointNet Model.\n",
    "ptresnet = get_model(arger, feature_extractor= AttentionPointResNet)\n",
    "respcr = iPCRNet(feature_model=ptresnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint, loading weights\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"backup/best_pointresnet.t7\"\n",
    "LOAD = True\n",
    "if os.path.isfile(best_model_path) and LOAD:\n",
    "    print(\"Found checkpoint, loading weights\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    respcr.load_state_dict(checkpoint['model'])\n",
    "    respcr.to(device)\n",
    "else:\n",
    "    print(\"notfound\")\n",
    "    start_epoch = 0\n",
    "    respcr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:32<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotation mse: [2888.81540847]\n",
      "translation mse: [1.6581904e-06]\n",
      "rotation mae: [30.4355322]\n",
      "translation mae: [1.6581904e-06]\n",
      "rotation error: [55.15109]\n",
      "translation error: [0.00200028]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(device, respcr, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PointNet Model.\n",
    "ptnet = get_model(arger, feature_extractor = PointNet)\n",
    "pcrnet = iPCRNet(feature_model=ptnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint, loading weights\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"backup/vanilla_pointnet_it3.t7\"\n",
    "LOAD = True\n",
    "if os.path.isfile(best_model_path) and LOAD:\n",
    "    print(\"Found checkpoint, loading weights\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    pcrnet.load_state_dict(checkpoint['model'])\n",
    "    pcrnet.to(device)\n",
    "else:\n",
    "    print(\"notfound\")\n",
    "    start_epoch = 0\n",
    "    pcrnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:25<00:00, 24.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotation mse: [126.0874611]\n",
      "translation mse: [7.859694e-05]\n",
      "rotation mae: [6.36654402]\n",
      "translation mae: [7.859694e-05]\n",
      "rotation error: [12.99217]\n",
      "translation error: [0.01199805]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(device, pcrnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
